{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Visualization\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Settings\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "use_gpu = True\n",
    "nfilters = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR Data Loading\n",
    "-------------------\n",
    "\n",
    "The CIFAR dataset contains 60000 32x32 colour images, split into 50000 training images and 10000 test images. Each image is labeled as one of either 10 (CIFAR10) or 100 (CIFAR100) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import color\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# augment the dataset by doing random crops and horizontal mirroring\n",
    "# this is done to prevent overfitting\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10('./data/CIFAR10', train=True, transform=transform_train, target_transform=None, download=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10('./data/CIFAR10', train=False, transform=transform_test, target_transform=None, download=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet Definition\n",
    "-----------------------\n",
    "In this convolutional net, we choose a large kernel size to have a more detailed visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 250666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=9, stride=1, padding=4) # out: 48 x 32 x 32\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1) # out: 48 x 16 x 16\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=9, stride=1, padding=4) # out: 96 x 16 x 16\n",
    "        self.conv3_bn = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # out: 96 x 8 x 8\n",
    "        self.conv4_bn = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # out: 96 x 8 x 8\n",
    "        self.conv5_bn = nn.BatchNorm2d(64)\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1) # out: 96 x 4 x 4\n",
    "        self.conv6_bn = nn.BatchNorm2d(64)\n",
    "#         self.do1 = nn.Dropout2d(p=0.5)\n",
    "        self.conv7 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=0) # out: 96 x 1 x 1\n",
    "        self.do1 = nn.Dropout2d(p=0.5)\n",
    "        self.fc1 = nn.Linear(64, 10) # 10 outputs: probability for each CIFAR10 class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolutional part\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
    "        x = F.relu(self.conv6_bn(self.conv6(x)))\n",
    "#         x = self.do1(x)\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.do1(x)\n",
    "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
    "        \n",
    "        # fully connected part\n",
    "        x = F.log_softmax(self.fc1(x), dim=1) # last activation is log softmax to get log class probabilities\n",
    "        \n",
    "        return x\n",
    "\n",
    "convnet = ConvNet()\n",
    "if use_gpu:\n",
    "    convnet = convnet.cuda()\n",
    "    \n",
    "# pick a subset of filters\n",
    "filter_inds = []\n",
    "for conv in [convnet.conv1, convnet.conv2, convnet.conv3, convnet.conv4]:\n",
    "    weights = conv.weight.data.view(-1, 1, conv.weight.size(2), conv.weight.size(3))\n",
    "    filter_inds.append(np.random.choice(weights.size(0), min(nfilters, weights.size(0)), replace=False))\n",
    "\n",
    "num_params = sum(p.numel() for p in convnet.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ConvNet\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch [1 / 150] average loss: 1.799924\n",
      "Epoch [2 / 150] average loss: 1.517398\n",
      "Epoch [3 / 150] average loss: 1.367723\n",
      "Epoch [4 / 150] average loss: 1.260014\n",
      "Epoch [5 / 150] average loss: 1.176993\n",
      "Epoch [6 / 150] average loss: 1.098565\n",
      "Epoch [7 / 150] average loss: 1.037168\n",
      "Epoch [8 / 150] average loss: 0.991804\n",
      "Epoch [9 / 150] average loss: 0.947552\n",
      "Epoch [10 / 150] average loss: 0.917389\n",
      "Epoch [11 / 150] average loss: 0.879763\n",
      "Epoch [12 / 150] average loss: 0.850048\n",
      "Epoch [13 / 150] average loss: 0.832831\n",
      "Epoch [14 / 150] average loss: 0.810646\n",
      "Epoch [15 / 150] average loss: 0.787724\n",
      "Epoch [16 / 150] average loss: 0.761502\n",
      "Epoch [17 / 150] average loss: 0.745570\n",
      "Epoch [18 / 150] average loss: 0.732379\n",
      "Epoch [19 / 150] average loss: 0.714851\n",
      "Epoch [20 / 150] average loss: 0.693571\n",
      "Epoch [21 / 150] average loss: 0.678094\n",
      "Epoch [22 / 150] average loss: 0.663216\n",
      "Epoch [23 / 150] average loss: 0.649116\n",
      "Epoch [24 / 150] average loss: 0.641621\n",
      "Epoch [25 / 150] average loss: 0.629625\n",
      "Epoch [26 / 150] average loss: 0.613759\n",
      "Epoch [27 / 150] average loss: 0.609035\n",
      "Epoch [28 / 150] average loss: 0.602782\n",
      "Epoch [29 / 150] average loss: 0.585998\n",
      "Epoch [30 / 150] average loss: 0.584499\n",
      "Epoch [31 / 150] average loss: 0.575050\n",
      "Epoch [32 / 150] average loss: 0.566937\n",
      "Epoch [33 / 150] average loss: 0.561615\n",
      "Epoch [34 / 150] average loss: 0.547016\n",
      "Epoch [35 / 150] average loss: 0.548351\n",
      "Epoch [36 / 150] average loss: 0.541871\n",
      "Epoch [37 / 150] average loss: 0.534635\n",
      "Epoch [38 / 150] average loss: 0.534648\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=convnet.parameters(), lr=learning_rate)\n",
    "\n",
    "# set to training mode\n",
    "convnet.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "filters = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    filters.append([])\n",
    "    \n",
    "    for image_batch, label_batch in train_dataloader:\n",
    "        \n",
    "        image_batch = Variable(image_batch)\n",
    "        label_batch = Variable(label_batch)\n",
    "        if use_gpu:\n",
    "            image_batch = image_batch.cuda()\n",
    "            label_batch = label_batch.cuda()\n",
    "        \n",
    "        # class predictions\n",
    "        prediction_batch = convnet(image_batch)\n",
    "        \n",
    "        # The cross-entropy loss.\n",
    "        # The first input are the predicted log class probabilities.\n",
    "        # The ground truth probabilites for each image are expected to be\n",
    "        # 1 for a single class and 0 for all other classes.\n",
    "        # This function expects as second input the index of the class with probability 1.\n",
    "        # (this function is not called cross-entropy, since this function assumes\n",
    "        # that the inputs are log probabilities, not probabilities).\n",
    "        loss = F.nll_loss(prediction_batch, label_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.data[0]\n",
    "        num_batches += 1\n",
    "        \n",
    "    # store filters learned in this epoch\n",
    "    for i, conv in enumerate([convnet.conv1, convnet.conv2, convnet.conv3, convnet.conv4]):\n",
    "        weights = conv.weight.data.view(-1, 1, conv.weight.size(2), conv.weight.size(3))\n",
    "        filters[-1].append(weights[filter_inds[i], :, :, :].cpu())\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average loss: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Curve\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: Load Pre-Trained Model\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convnet.load_state_dict(torch.load('./pretrained/feature_vis_convnet.pth'))\n",
    "\n",
    "# this is how the model parameters can be saved:\n",
    "# torch.save(convnet.state_dict(), './pretrained/my_feature_vis_convnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the Test Set\n",
    "-------------------------\n",
    "\n",
    "The best current methods achieve a classification error percentage of around 3.47%. See [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130) for a leaderboard.\n",
    "However, due to its small size and few classes, CIFAR10 is not challenging enough anymore to be used as a serious benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to evaluation mode\n",
    "convnet.eval()\n",
    "\n",
    "num_incorrect = 0\n",
    "test_loss_avg = 0\n",
    "num_batches = 0\n",
    "num_instances = 0\n",
    "for image_batch, label_batch in test_dataloader:\n",
    "    \n",
    "    image_batch = Variable(image_batch)\n",
    "    label_batch = Variable(label_batch)\n",
    "    if use_gpu:\n",
    "        image_batch = image_batch.cuda()\n",
    "        label_batch = label_batch.cuda()\n",
    "    \n",
    "    # class predictions\n",
    "    prediction_batch = convnet(image_batch)\n",
    "    \n",
    "    # get number of correct and incorrect class predictions\n",
    "    _, predicted_label = prediction_batch.max(dim=1)\n",
    "    num_incorrect += (predicted_label != label_batch).sum().data[0]\n",
    "\n",
    "    # cross-entropy loss\n",
    "    loss = F.nll_loss(prediction_batch, label_batch)\n",
    "\n",
    "    test_loss_avg += loss.data[0]\n",
    "    num_batches += 1\n",
    "    num_instances += image_batch.size(0)\n",
    "    \n",
    "test_loss_avg /= num_batches\n",
    "print('average loss: %f' % (test_loss_avg))\n",
    "print('classification error: %f%%' % ((num_incorrect / num_instances)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Visualizations\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "def filter_collage(filter_weights, nrow):\n",
    "#     filter_weights = filter_weights.view(-1, 1, filter_weights.size(2), filter_weights.size(3))\n",
    "#     filter_weights = filter_weights[:nmax, :, :, :]\n",
    "    filter_max = filter_weights.view(filter_weights.size(0), 1, -1).max(dim=2)[0].view(filter_weights.size(0), 1, 1, 1)\n",
    "    filter_min = filter_weights.view(filter_weights.size(0), 1, -1).min(dim=2)[0].view(filter_weights.size(0), 1, 1, 1)\n",
    "    filter_weights = (filter_weights - filter_min) / (filter_max - filter_min)\n",
    "    return torchvision.utils.make_grid(filter_weights, nrow=nrow)\n",
    "    \n",
    "def show_tensor(tensor):\n",
    "    fig, ax = plt.subplots(figsize=(7, 7), nrows=1, ncols=1)\n",
    "    ax.imshow(np.transpose(tensor.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "show_tensor(filter_collage(filters[-1][0], nrow=10))\n",
    "show_tensor(filter_collage(filters[-1][1], nrow=10))\n",
    "show_tensor(filter_collage(filters[-1][2], nrow=10))\n",
    "show_tensor(filter_collage(filters[-1][3], nrow=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
